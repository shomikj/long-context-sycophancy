{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44c4f67-5628-4852-9a6f-33fd3d66a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query import build_context, QueryClaude, QueryGPT, QueryGPTJudge, QueryGemini, QueryVertexAI, GPT_4_1_MINI, GPT_5, GEMINI_PRO, LLAMA_SCOUT, CLAUDE_SONNET_4\n",
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import fsspec\n",
    "\n",
    "MODELS = {\n",
    "    GPT_5: QueryGPT(GPT_5),\n",
    "    GEMINI_PRO: QueryGemini(GEMINI_PRO),\n",
    "    LLAMA_SCOUT: QueryVertexAI(LLAMA_SCOUT)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add143f4-8ef3-4e25-9e60-dea83db4d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv(\"../data/prompts.csv\")\n",
    "prompts = prompts[prompts[\"prompt_id\"].str.startswith(\"aita\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683695c-d239-4d86-a84e-a4a9118473c7",
   "metadata": {},
   "source": [
    "# Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a0591-d18a-4b52-8bf0-d4a1bbe53af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ZERO_SHOT = 1\n",
    "\n",
    "#MODELS[GPT_5].build_and_run_batch(prompts, [\"zero-shot\"], [[]], NUM_ZERO_SHOT)\n",
    "#MODELS[GEMINI_PRO].build_and_run_batch(prompts, [\"zero-shot\"], [[]], NUM_ZERO_SHOT)\n",
    "#MODELS[LLAMA_SCOUT].build_and_run_batch(prompts, [\"zero-shot\"], [[]], NUM_ZERO_SHOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a8e85-8233-47d2-a79e-83ad8d783596",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_SCOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276768bf-49a9-496f-a7ec-0aa618a59704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batches = {\n",
    "    #GPT_5: \"batch_6913421f99b08190a6b758ec627d6199\",\n",
    "    #GEMINI_PRO: \"projects/451496912721/locations/global/batchPredictionJobs/9112418119472644096\"\n",
    "    LLAMA_SCOUT: \"projects/451496912721/locations/us-central1/batchPredictionJobs/1550619309959544832\"\n",
    "}\n",
    "\n",
    "for k,v in model_batches.items():\n",
    "    results = MODELS[k].get_batch_results(v) \n",
    "    #results.to_csv(f\"../data/responses/aita_without_context_{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86017d84-3529-424a-bb4d-25974a6a2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'{\"name\": \"projects/451496912721/locations/us-central1/batchPredictionJobs/1550619309959544832\", \"displayName\": \"5fcdb22e-2ca3-4d3e-9f09-beb92a862d20\", \"model\": \"publishers/meta/models/llama-4-scout-17b-16e-instruct-maas\", \"inputConfig\": {\"instancesFormat\": \"jsonl\", \"gcsSource\": {\"uris\": [\"gs://sj-batch-eval/batch_input_5fcdb22e-2ca3-4d3e-9f09-beb92a862d20.jsonl\"]}}, \"outputConfig\": {\"predictionsFormat\": \"jsonl\", \"gcsDestination\": {\"outputUriPrefix\": \"gs://sj-batch-eval/{batch_id}\"}}, \"state\": \"JOB_STATE_PENDING\", \"createTime\": \"2025-11-11T20:32:07.277726Z\", \"updateTime\": \"2025-11-11T20:32:07.277726Z\", \"encryptionSpec\": {}, \"modelVersionId\": \"1\"}'\n",
    "\n",
    "\n",
    "from query import get_gcloud_access_token\n",
    "access_token = get_gcloud_access_token()\n",
    "import requests\n",
    "project_id = \"projects/451496912721/locations/us-central1/batchPredictionJobs/1550619309959544832\"\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/{project_id}\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0deee0d-bbdd-4357-a4db-23b018a4f4e6",
   "metadata": {},
   "source": [
    "# With-Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de7365-8412-49dd-b7c2-7f9b23c8dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv(\"../data/interactions.csv\") \n",
    "participants = pd.read_csv(\"../data/participants.csv\")\n",
    "interactions = interactions[interactions[\"user_id\"].isin(participants[\"user_id\"])]\n",
    "\n",
    "context_ids = []\n",
    "contexts = []\n",
    "\n",
    "for user_id in interactions[\"user_id\"].unique():\n",
    "    user_interaction = interactions[interactions[\"user_id\"]==user_id]\n",
    "    user_interaction = user_interaction.sort_values(by=\"timestamp\")\n",
    "\n",
    "    #for cutoff in [10000, 20000, 30000, 40000, 50000]:\n",
    "    #    if cutoff > user_interaction[\"tokens\"].max():\n",
    "    #        continue\n",
    "    #    interaction_chunk = user_interaction[user_interaction[\"tokens\"]<=cutoff]\n",
    "    #   context_ids.append(user_id+\"-\"+str(int(interaction_chunk[\"tokens\"].max())))\n",
    "    #   contexts.append(build_context(interaction_chunk))\n",
    "\n",
    "    context_ids.append(user_id+\"-\"+str(user_interaction[\"tokens\"].max()))\n",
    "    contexts.append(build_context(user_interaction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c4c58-77ff-45e3-9779-28c1ce116db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 3\n",
    "\n",
    "\n",
    "MODELS[GPT_5].build_and_run_batch(prompts, context_ids, contexts, NUM_ITERATIONS)\n",
    "#MODELS[GEMINI_PRO].build_and_run_batch(prompts, context_ids, contexts, NUM_ITERATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d3f54-0f59-4fbb-a686-48ce73385094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batches = {\n",
    "    #GPT_5: [\"batch_69134a7f1ea0819082c3b89cdd851025\", \"batch_6913954189988190b3d84565ee8c3f0a\"],\n",
    "    #GEMINI_PRO: [\"projects/451496912721/locations/global/batchPredictionJobs/4892545268626489344\"]\n",
    "}\n",
    "for model,files in model_batches.items():\n",
    "    results = []\n",
    "    for file in files:\n",
    "        results.append(MODELS[model].get_batch_results(file))\n",
    "    results = pd.concat(results)\n",
    "    results.to_csv(f\"../data/responses/aita_with_context_{model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aef874-5a1b-4552-8e2f-5073a928d8a0",
   "metadata": {},
   "source": [
    "# Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f133b5-5cd7-4bf2-b421-ebce402fb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/responses.csv\")\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "df = df[df[\"model\"]==MODEL]\n",
    "df = df[df[\"prompt_id\"].str.startswith(\"aita\")]\n",
    "participants = pd.read_csv(\"../data/participants.csv\")\n",
    "participants = participants[participants[\"passed_attention\"]==\"yes\"]\n",
    "df = df[df[\"iteration\"]<=37]\n",
    "with_context = df[(df[\"context\"].isin(participants[\"user_id\"]))]\n",
    "with_context.to_csv(f\"../data/responses/aita_with_context_{MODEL}.csv\", index=False)\n",
    "without_context = df[(df[\"context\"]==\"zero-shot\")]\n",
    "without_context.to_csv(f\"../data/responses/aita_without_context_{MODEL}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d2249-958a-42f2-b045-fb37d9771d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1-mini-2025-04-14\"\n",
    "df = pd.concat([pd.read_csv(f\"../data/responses/aita_with_context_{MODEL}.csv\"), \n",
    "      pd.read_csv(f\"../data/responses/aita_without_context_{MODEL}.csv\")])\n",
    "judge = QueryGPTJudge()\n",
    "#judge.build_and_run_batch(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29175fde-16f1-4cb9-be71-64179b574e34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m judge = QueryGPTJudge()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model,batch \u001b[38;5;129;01min\u001b[39;00m model_batches.items():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     results = \u001b[43mjudge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_batch_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     11\u001b[39m     results.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../data/judge_responses/aita_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIT/Projects/Alignment_Drift/long-context-eval/query.py:242\u001b[39m, in \u001b[36mQueryGPTJudge.get_batch_results\u001b[39m\u001b[34m(self, batch_id)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_batch_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_id):\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_batch_results()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIT/Projects/Alignment_Drift/long-context-eval/query.py:213\u001b[39m, in \u001b[36mQueryGPTJudge.download_batch\u001b[39m\u001b[34m(self, batch_id)\u001b[39m\n\u001b[32m    211\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.client.batches.retrieve(batch_id)\n\u001b[32m    212\u001b[39m output_file_id = batch.output_file_id\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgpt_results.jsonl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    216\u001b[39m     f.write(response.content.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/main/lib/python3.13/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/main/lib/python3.13/warnings.py:637\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    636\u001b[39m     warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/main/lib/python3.13/site-packages/openai/resources/files.py:314\u001b[39m, in \u001b[36mFiles.retrieve_content\u001b[39m\u001b[34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03mReturns the contents of the specified file.\u001b[39;00m\n\u001b[32m    303\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_id:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get(\n\u001b[32m    316\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/content\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    317\u001b[39m     options=make_request_options(\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m     cast_to=\u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    321\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "model_batches = {\n",
    "    #GPT_5: \"batch_6913a589c0f481908ce9a3d8060c7982\",\n",
    "    #GEMINI_PRO: \"batch_6913a648ee048190a20c337bb5505838\",\n",
    "    GPT_4_1_MINI: \"batch_6913c046d0608190a063ff5e2b29aed8\"\n",
    "    #CLAUDE_SONNET_4: \"batch_6913c036f4808190a026ad0546a0650f\"\n",
    "}\n",
    "judge = QueryGPTJudge()\n",
    "\n",
    "for model,batch in model_batches.items():\n",
    "    results = judge.get_batch_results(batch) \n",
    "    results.to_csv(f\"../data/judge_responses/aita_{model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18f0efaa-d8a2-4c34-b490-57c8a5d9fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\n",
    "    GEMINI_PRO,\n",
    "    GPT_5,\n",
    "    CLAUDE_SONNET_4\n",
    "]\n",
    "\n",
    "responses = []\n",
    "judge_labels = []\n",
    "for m in MODEL_NAMES:\n",
    "    responses.append(pd.read_csv(f\"../data/responses/aita_with_context_{m}.csv\"))\n",
    "    responses.append(pd.read_csv(f\"../data/responses/aita_without_context_{m}.csv\"))\n",
    "    judge_labels.append(pd.read_csv(f\"../data/judge_responses/aita_{m}.csv\"))\n",
    "\n",
    "responses = pd.concat(responses)\n",
    "judge_labels = pd.concat(judge_labels)\n",
    "\n",
    "responses[\"id\"] = responses[\"prompt_id\"] + responses[\"context\"] + responses[\"model\"] + responses[\"iteration\"].astype(str)\n",
    "judge_labels[\"id\"] = judge_labels[\"prompt_id\"] + judge_labels[\"context\"] + judge_labels[\"model\"] + judge_labels[\"iteration\"].astype(str)\n",
    "df = responses.merge(judge_labels[[\"id\", \"gpt_judge\"]], on=[\"id\"], how=\"inner\")\n",
    "df = df.drop(columns=[\"id\"])\n",
    "df.to_csv(\"../data/responses_aita.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d25c9b-ea2b-4917-83ef-8600d5cd945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                     context\n",
       "claude-sonnet-4-20250514  0          0.642105\n",
       "                          1          0.593860\n",
       "gemini-2.5-pro            0          0.700000\n",
       "                          1          0.600877\n",
       "gpt-5-2025-08-07          0          0.536842\n",
       "                          1          0.559649\n",
       "Name: gpt_judge, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv\n",
    "df[\"context\"] = df[\"context\"].str.strip()\n",
    "df[\"context\"] = (df[\"context\"]==\"zero-shot\").astype(int)\n",
    "df[\"gpt_judge\"] = df[\"gpt_judge\"]\n",
    "df.groupby([\"model\", \"context\"])[\"gpt_judge\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee7744-b89e-4851-9cea-afa31ab149a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
