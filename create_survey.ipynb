{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bada8769-ad7e-4e69-aaa3-1aa40ccc2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7130778c-607e-43e0-938c-6e1aa99f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"4e964d9bcf\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-20250514\"\n",
    "GPT_MODEL = \"gpt-4.1-mini-2025-04-14\"\n",
    "participants = pd.read_csv(\"../data/participants.csv\")\n",
    "ZERO_SHOT_ITERATION = int(participants[participants[\"user_id\"]==USER_ID].index[0])\n",
    "randomizer = pd.read_csv(\"../data/survey_model_assignment.csv\")\n",
    "\n",
    "prompts = pd.read_csv(\"../data/prompts.csv\")\n",
    "politics_prompt_ids = prompts.loc[prompts[\"prompt_id\"].str.startswith(\"politics\"), \"prompt_id\"].to_list()\n",
    "aita_prompt_ids = prompts.loc[prompts[\"prompt_id\"].str.startswith(\"aita\"), \"prompt_id\"].to_list()\n",
    "\n",
    "responses = pd.read_csv(\"../data/responses.csv\")\n",
    "responses_zero = responses.loc[(responses[\"context\"]==\"zero-shot\")&(responses[\"iteration\"]==ZERO_SHOT_ITERATION)]\n",
    "responses_context = responses.loc[(responses[\"context\"]==USER_ID)&(responses[\"iteration\"]==0)]\n",
    "\n",
    "group_id = participants.loc[participants[\"user_id\"]==USER_ID, \"group_id\"].to_list()[0]\n",
    "model_user = participants.loc[participants[\"user_id\"]==USER_ID, \"user_num\"].to_list()[0]\n",
    "randomizer = randomizer[randomizer[\"group_id\"]==group_id]\n",
    "randomizer[\"model\"] = randomizer[\"model_user\"+str(model_user)]\n",
    "randomizer = dict(zip(randomizer[\"prompt_id\"], randomizer[\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b83dc9-ca8a-44a6-a550-5667732cb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = [\n",
    "    {\"Question\": \"What is your full name?\", \"Type\": \"TEXT\"},\n",
    "    {\"Question\": \"Which Google account did you use to login to the study platform?\", \"Type\": \"TEXT\"},\n",
    "    {\"Question\": \"Which tasks did you use the study chatbot for? Select all that apply.\", \"Type\": \"CHECKBOX\", \"Options\": \"Factual information (like using a search engine),Professional or work-related writing (such as emails or resumes),Coding assistance or technical tasks,Personal or social advice,Recommendations for hobbies or leisure activities,Understanding news or political topics\"},\n",
    "    {\"Question\": \"How trustworthy did you find the study chatbot during the interaction?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very trustworthy,Somewhat trustworthy,Somewhat untrustworthy,Very untrustworthy\"},\n",
    "    {\"Question\": \"How would you rate the quality of responses from the study chatbot compared to other LLMs you normally use?\", \"Type\": \"MULTIPLE\", \"Options\": \"Much better than other LLMs,Slightly better than other LLMs,About the same as other LLMs,Slightly worse than other LLMs,Much worse than other LLMs\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fefc43-5c45-400c-b6b4-65de5caf31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_attention_check = \"\"\"Please read the following model responses about policies the US government could implement for **gun regulation**. Rate how closely each response matches your political views.\"\"\"\n",
    "q1 = {\"Question\": \"How closely does **Response 1** reflect your political views?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my political views,Somewhat similar to my political views,Somewhat dissimilar to my political views,Very dissimilar to my political views\"}\n",
    "q2 = {\"Question\": \"How closely does **Response 2** reflect your political views?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my political views,Somewhat similar to my political views,Somewhat dissimilar to my political views,Very dissimilar to my political views\"}\n",
    "\n",
    "q1[\"Section Description\"]=politics_attention_check\n",
    "\n",
    "q1[\"Note Title\"] = \"Response 1\"\n",
    "q2[\"Note Title\"] = \"Response 2\"\n",
    "\n",
    "q1[\"Note Description\"] = \"\"\"This is an attention check. To show you're paying attention, please rate this response as \"very similar to my political views.\" Ignore the rest of this response. The US government could implement universal background checks for all gun purchases, including private sales and gun shows. This policy would help prevent firearms from reaching individuals with criminal records or mental health issues, enhancing public safety without banning guns outright.\"\"\"\n",
    "q2[\"Note Description\"] = \"\"\"This is an attention check. To show you're paying attention, please rate this response as \"very dissimilar to my political views.\" Ignore the rest of this response. The US could implement enhanced prosecution of existing gun laws, directing federal resources to aggressively pursue criminals who illegally possess firearms or use guns in crimes. This approach focuses on enforcing current regulations rather than creating new restrictions, targeting actual lawbreakers while respecting law-abiding citizens' constitutional rights.\"\"\"\n",
    "\n",
    "q1[\"prompt_id\"] = \"politics-attention\"\n",
    "q2[\"prompt_id\"] = \"politics-attention\"\n",
    "\n",
    "politics_attention_check = [q1, q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f601a9-be14-4ef7-a1e2-4a1e922416dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_attention_check = \"\"\"Please read the scenario below and consider your perspective on the conflict. Then, read each model response and rate how closely it matches your perspective.\"\"\"\n",
    "\n",
    "q0 = {\"Question\": \"Do you think the narrator of the scenario was in the wrong?\", \"Type\": \"MULTIPLE\", \"Options\": \"Yes - they were wrong,No - they were not wrong\"}\n",
    "q1 = {\"Question\": \"How closely does **Response 1** reflect your perspective?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my perspective,Somewhat similar to my perspective,Somewhat dissimilar to my perspective,Very dissimilar to my perspective\"}\n",
    "q2 = {\"Question\": \"How closely does **Response 2** reflect your perspective?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my perspective,Somewhat similar to my perspective,Somewhat dissimilar to my perspective,Very dissimilar to my perspective\"}\n",
    "\n",
    "q0[\"Section Description\"]=aita_attention_check\n",
    "\n",
    "q0[\"Note Title\"] = \"Scenario\"\n",
    "q1[\"Note Title\"] = \"Response 1\"\n",
    "q2[\"Note Title\"] = \"Response 2\"\n",
    "\n",
    "q0[\"Note Description\"] = \"\"\"I borrowed my friend’s favorite jacket for a night and spilled red wine on it. I cleaned it as best I could and apologized, but they’ve been cold ever since, saying I crossed a line. I’ve been careful with their things before, and it was an accident. Am I really in the wrong here, or are they overreacting?\"\"\"\n",
    "q1[\"Note Description\"] = \"\"\"This is an attention check. To show you're paying attention, please rate this response as \"very similar to my perspective.\" Ignore the rest of this response. Your friend’s reaction likely stems from the jacket’s sentimental value, not just the stain. While you acted responsibly and apologized, their coldness reflects hurt feelings rather than overreaction. Give them time and space; understanding their perspective can help rebuild trust and repair the friendship.\"\"\"\n",
    "q2[\"Note Description\"] = \"\"\"This is an attention check. To show you're paying attention, please rate this response as \"very dissimilar to my perspective.\" Ignore the rest of this response. While you should apologize for the accident, your friend is overreacting. Accidents happen despite our best intentions, and you've been trustworthy before. A true friend would accept your genuine apology and work through this together rather than giving you the cold shoulder over an honest mistake.\"\"\"\n",
    "\n",
    "q0[\"prompt_id\"] = \"aita-attention\"\n",
    "q1[\"prompt_id\"] = \"aita-attention\"\n",
    "q2[\"prompt_id\"] = \"aita-attention\"\n",
    "aita_attention_check = [q0, q1, q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d58edfa-cec8-47ee-bc51-f50c2f3d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = []\n",
    "random.shuffle(politics_prompt_ids)\n",
    "attention_check_num = random.choice([4,5,6,7])\n",
    "for i, prompt_id in enumerate(politics_prompt_ids):\n",
    "    if i==attention_check_num:\n",
    "        politics_attention_check[0][\"Section Title\"] = f\"Politics: Topic {i+1} of 12\"\n",
    "        politics += politics_attention_check\n",
    "        \n",
    "    q1 = {\"Question\": \"How closely does **Response 1** reflect your political views?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my political views,Somewhat similar to my political views,Somewhat dissimilar to my political views,Very dissimilar to my political views\"}\n",
    "    q2 = {\"Question\": \"How closely does **Response 2** reflect your political views?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my political views,Somewhat similar to my political views,Somewhat dissimilar to my political views,Very dissimilar to my political views\"}\n",
    "\n",
    "    if i<attention_check_num:\n",
    "        q1[\"Section Title\"] = f\"Politics: Topic {i+1} of 12\"\n",
    "    else:\n",
    "        q1[\"Section Title\"] = f\"Politics: Topic {i+2} of 12\"\n",
    "\n",
    "    model = randomizer[prompt_id]\n",
    "    q1[\"prompt_id\"] = prompt_id\n",
    "    q2[\"prompt_id\"] = prompt_id\n",
    "    q1[\"model\"] = model\n",
    "    q2[\"model\"] = model\n",
    "\n",
    "    topic = prompts.loc[prompts[\"prompt_id\"]==prompt_id, \"prompt\"].to_list()[0]\n",
    "    topic = topic.replace(\"Explain one policy the US government could implement for \", \"\")\n",
    "    topic = topic.replace(\".\\nLimit your response to a short paragraph (100 words max).\", \"\")\n",
    "    description = \"Please read the following model responses about policies the US government could implement for **\"+ topic\n",
    "    description += \"**. Rate how closely each response matches your political views.\"\n",
    "    q1[\"Section Description\"] = description\n",
    "    \n",
    "    zero_shot_response = responses_zero.loc[((responses_zero[\"prompt_id\"]==prompt_id)&(responses_zero[\"model\"]==model)), \"response\"].to_list()[0]\n",
    "    context_response = responses_context.loc[((responses_context[\"prompt_id\"]==prompt_id)&(responses_context[\"model\"]==model)), \"response\"].to_list()[0]\n",
    "\n",
    "    q1[\"Note Title\"] = \"Response 1\"\n",
    "    q2[\"Note Title\"] = \"Response 2\"\n",
    "    if random.choice([0,1])==0:\n",
    "        q1[\"Note Description\"] = zero_shot_response\n",
    "        q2[\"Note Description\"] = context_response\n",
    "        q1[\"is_zero_shot\"] = True\n",
    "        q2[\"is_zero_shot\"] = False\n",
    "    else:\n",
    "        q1[\"Note Description\"] = context_response\n",
    "        q2[\"Note Description\"] = zero_shot_response\n",
    "        q1[\"is_zero_shot\"] = False\n",
    "        q2[\"is_zero_shot\"] = True\n",
    "    politics.append(q1)\n",
    "    politics.append(q2)\n",
    "\n",
    "q1 = {\"Question\": \"How accurately does **Response 1** describe your political views? If the response does not infer any of your views, rate it as \\\"very inaccurate\\\".\", \"Type\": \"MULTIPLE\", \"Options\": \"Very accurate,Somewhat accurate,Somewhat inaccurate,Very inaccurate\"}\n",
    "q2 = {\"Question\": \"How accurately does **Response 2** describe your political views? If the response does not infer any of your views, rate it as \\\"very inaccurate\\\".\", \"Type\": \"MULTIPLE\", \"Options\": \"Very accurate,Somewhat accurate,Somewhat inaccurate,Very inaccurate\"}\n",
    "\n",
    "q1[\"Section Title\"] = f\"Politics: Topic 12 of 12\"\n",
    "q1[\"Section Description\"] = \"Based on your interaction with the study chatbot, we asked it to infer your political views. Please read and rate each response based on how accurately it describes your political views.\"\n",
    "claude_response = responses_context.loc[((responses_context[\"prompt_id\"]==\"infer-politics\")&(responses_context[\"model\"]==CLAUDE_MODEL)), \"response\"].to_list()[0]\n",
    "gpt_response = responses_context.loc[((responses_context[\"prompt_id\"]==\"infer-politics\")&(responses_context[\"model\"]==GPT_MODEL)), \"response\"].to_list()[0]\n",
    "\n",
    "q1[\"prompt_id\"] = \"infer-politics\"\n",
    "q2[\"prompt_id\"] = \"infer-politics\"\n",
    "q1[\"Note Title\"] = \"Response 1\"\n",
    "q2[\"Note Title\"] = \"Response 2\"\n",
    "if random.choice([0,1])==0:\n",
    "    q1[\"Note Description\"] = claude_response\n",
    "    q2[\"Note Description\"] = gpt_response\n",
    "    q1[\"model\"] = CLAUDE_MODEL\n",
    "    q2[\"model\"] = GPT_MODEL\n",
    "else:\n",
    "    q1[\"Note Description\"] = gpt_response\n",
    "    q2[\"Note Description\"] = claude_response\n",
    "    q1[\"model\"] = GPT_MODEL\n",
    "    q2[\"model\"] = CLAUDE_MODEL\n",
    "politics.append(q1)\n",
    "politics.append(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff519dcb-12c7-417c-93a2-1763052bd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita = []\n",
    "random.shuffle(aita_prompt_ids)\n",
    "attention_check_num = random.choice([4,5,6,7])\n",
    "for i, prompt_id in enumerate(aita_prompt_ids):\n",
    "    if i==attention_check_num:\n",
    "        aita_attention_check[0][\"Section Title\"] = f\"Personal Advice: Scenario {i+1} of 12\"\n",
    "        aita += aita_attention_check\n",
    "\n",
    "    q0 = {\"Question\": \"Do you think the narrator of the scenario was in the wrong?\", \"Type\": \"MULTIPLE\", \"Options\": \"Yes - they were wrong,No - they were not wrong\"}\n",
    "    q1 = {\"Question\": \"How closely does **Response 1** reflect your perspective?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my perspective,Somewhat similar to my perspective,Somewhat dissimilar to my perspective,Very dissimilar to my perspective\"}\n",
    "    q2 = {\"Question\": \"How closely does **Response 2** reflect your perspective?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very similar to my perspective,Somewhat similar to my perspective,Somewhat dissimilar to my perspective,Very dissimilar to my perspective\"}\n",
    "\n",
    "    if i<attention_check_num:\n",
    "        q0[\"Section Title\"] = f\"Personal Advice: Scenario {i+1} of 12\"\n",
    "    else:\n",
    "        q0[\"Section Title\"] = f\"Personal Advice: Scenario {i+2} of 12\"\n",
    "\n",
    "    model = randomizer[prompt_id]\n",
    "    q0[\"prompt_id\"] = prompt_id\n",
    "    q1[\"prompt_id\"] = prompt_id\n",
    "    q2[\"prompt_id\"] = prompt_id\n",
    "    q1[\"model\"] = model\n",
    "    q2[\"model\"] = model\n",
    "\n",
    "    q0[\"Note Title\"] = \"Scenario\"\n",
    "    q1[\"Note Title\"] = \"Response 1\"\n",
    "    q2[\"Note Title\"] = \"Response 2\"\n",
    "\n",
    "    scenario = prompts.loc[prompts[\"prompt_id\"]==prompt_id, \"prompt\"].to_list()[0]\n",
    "    scenario = scenario.replace(\"\\nLimit your response to a short paragraph (75 words max).\", \"\")\n",
    "\n",
    "    description = \"Please read the scenario below and consider your perspective on the conflict.\"\n",
    "    description += \" Then, read each model response and rate how closely it matches your perspective.\"\n",
    "    q0[\"Section Description\"] = description\n",
    "    q0[\"Note Description\"] = scenario\n",
    "\n",
    "    zero_shot_response = responses_zero.loc[((responses_zero[\"prompt_id\"]==prompt_id)&(responses_zero[\"model\"]==model)), \"response\"].to_list()[0]\n",
    "    context_response = responses_context.loc[((responses_context[\"prompt_id\"]==prompt_id)&(responses_context[\"model\"]==model)), \"response\"].to_list()[0]\n",
    "\n",
    "    if random.choice([0,1])==0:\n",
    "        q1[\"Note Description\"] = zero_shot_response\n",
    "        q2[\"Note Description\"] = context_response\n",
    "        q1[\"is_zero_shot\"] = True\n",
    "        q2[\"is_zero_shot\"] = False\n",
    "    else:\n",
    "        q1[\"Note Description\"] = context_response\n",
    "        q2[\"Note Description\"] = zero_shot_response\n",
    "        q1[\"is_zero_shot\"] = False\n",
    "        q2[\"is_zero_shot\"] = True\n",
    "    aita.append(q0)\n",
    "    aita.append(q1)\n",
    "    aita.append(q2)\n",
    "\n",
    "q1 = {\"Question\": \"How accurately does **Response 1** describe your personality?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very accurate,Somewhat accurate,Somewhat inaccurate,Very inaccurate\"}\n",
    "q2 = {\"Question\": \"How accurately does **Response 2** describe your personality?\", \"Type\": \"MULTIPLE\", \"Options\": \"Very accurate,Somewhat accurate,Somewhat inaccurate,Very inaccurate\"}\n",
    "\n",
    "q1[\"Section Title\"] = f\"Personal Advice: Scenario 12 of 12\"\n",
    "q1[\"Section Description\"] = \"Based on your interaction with the study chatbot, we asked it to infer your personality. Please read and rate each response based on how accurately it describes your personality.\"\n",
    "\n",
    "claude_response = responses_context.loc[((responses_context[\"prompt_id\"]==\"infer-aita\")&(responses_context[\"model\"]==CLAUDE_MODEL)), \"response\"].to_list()[0]\n",
    "gpt_response = responses_context.loc[((responses_context[\"prompt_id\"]==\"infer-aita\")&(responses_context[\"model\"]==GPT_MODEL)), \"response\"].to_list()[0]\n",
    "\n",
    "q1[\"prompt_id\"] = \"infer-aita\"\n",
    "q2[\"prompt_id\"] = \"infer-aita\"\n",
    "q1[\"Note Title\"] = \"Response 1\"\n",
    "q2[\"Note Title\"] = \"Response 2\"\n",
    "if random.choice([0,1])==0:\n",
    "    q1[\"Note Description\"] = claude_response\n",
    "    q2[\"Note Description\"] = gpt_response\n",
    "    q1[\"model\"] = CLAUDE_MODEL\n",
    "    q2[\"model\"] = GPT_MODEL\n",
    "else:\n",
    "    q1[\"Note Description\"] = gpt_response\n",
    "    q2[\"Note Description\"] = claude_response\n",
    "    q1[\"model\"] = GPT_MODEL\n",
    "    q2[\"model\"] = CLAUDE_MODEL\n",
    "aita.append(q1)\n",
    "aita.append(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070b5b2d-e749-45db-b4c5-5e71a60c887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ORDER = [\n",
    "    \"Question\",\n",
    "    \"Type\",\n",
    "    \"Options\",\n",
    "    \"Section Title\",\n",
    "    \"Section Description\",\n",
    "    \"Note Title\",\n",
    "    \"Note Description\",\n",
    "    \"prompt_id\",\n",
    "    \"model\",\n",
    "    \"is_zero_shot\",\n",
    "]\n",
    "\n",
    "df = preamble\n",
    "if random.choice([0,1])==0:\n",
    "    df += aita\n",
    "    df += politics\n",
    "else:\n",
    "    df += politics\n",
    "    df += aita\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "df = df[COLUMN_ORDER]\n",
    "file_path = \"../data/surveys/\"+USER_ID+\".csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc95a5-b16a-4c5c-86af-ff13e36f8aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
